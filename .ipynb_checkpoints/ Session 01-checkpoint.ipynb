{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82688c4d-cf49-4cee-afbe-90e4c6bab532",
   "metadata": {},
   "source": [
    "# Learning Pyspark - Session 01\n",
    "\n",
    "In the session, we'll be covering the following topics:\n",
    "\n",
    "1. What is PySpark?\n",
    "2. How to check the version of PySpark installed on the local PC?\n",
    "3. What is SparkContext?\n",
    "4. What is an RDD in PySpark?\n",
    "\n",
    "    4.1 How to create a RDD using PySpark?\n",
    "    \n",
    "    4.2 How to read a `.txt` file as a PySpark RDD?\n",
    "\n",
    "5. What is a PySpark DataFrame?\n",
    "\n",
    "    5.1. How to check the schema?\n",
    "    \n",
    "    5.2. How to read a `.csv` file as a PySpark DataFrame?\n",
    "    \n",
    "    5.3. How to check the shape of PySpark DataFrame?\n",
    "    \n",
    "    5.4. How to see the first 5 rows of a PySpark DataFrame?\n",
    "    \n",
    "    5.5. How to see the last 5 rows of a PySpark DataFrame?\n",
    "    \n",
    "    5.6. How to subset columns from a PySpark DataFrame?\n",
    "    \n",
    "    5.7. How to subset rows from a PySpark DataFrame?\n",
    "    \n",
    "    5.8. How to generate summary statistics from a PySpark DataFrame?\n",
    "    \n",
    "    5.9. How to add a new column to a PySpark DataFrame?\n",
    "    \n",
    "    5.10. How to perform groupby operations on a PySpark DataFrame?\n",
    "\n",
    "## 1. What is PySpark?\n",
    "\n",
    "PySpark is an interface for Apache Spark in Python. It allows the user to write Spark applications using Python API and provides features to analyze data in a distributed environment. PySpark supports the following features:\n",
    "\n",
    "1. Spark SQL\n",
    "2. DataFrame\n",
    "3. Streaming\n",
    "4. MLlib\n",
    "5. Spark Core\n",
    "\n",
    "<img src='./images/pyspark-components.png'/>\n",
    "\n",
    "#### 1. Spark SQL and DataFrame:\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrame and can also act as distributed SQL query engine.\n",
    "\n",
    "#### 2. pandas API on Spark:\n",
    "\n",
    "Pandas API on Spark allows you to scale your pandas workload out. This lets you seemlessly convert a pandas DataFrame to a pyspark DataFrame.\n",
    "\n",
    "#### 3. Streaming:\n",
    "\n",
    "Streaming feature in Apache Spark enables the user to analyze streaming data with fault tolerance characterstics.\n",
    "\n",
    "#### 4. MLlib:\n",
    "\n",
    "\n",
    "A scalable machine learning library that provies set of APIs that help user create and tune practical machine learning piplelines.\n",
    "\n",
    "#### 5. Spark Core:\n",
    "\n",
    "It provides an RDD (Resilient Distributed Dataset) and in-memory computing capabilities.\n",
    "\n",
    "## 2. How to check the version of PySpark installed on the local PC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fb6417-b6ac-4ea7-9885-c3c0d5a61ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7cf66-5c80-47c3-ba8e-b9fb35c84601",
   "metadata": {},
   "source": [
    "## 3. What is SparkContext?\n",
    "\n",
    "**Spark Context** is the entry point of the Apache Spark functionality. It enables the user/code to access the Spark cluster with the help of a Resource Manager (YARN/Mesos or Other). It is the first step before you can use Apache Spark and it helps us utilise all of the Spark's capabilities.\n",
    "\n",
    "<img src='./images/spark_context_01.jpg'/>\n",
    "\n",
    "Following are the parameters of a SparkContext:\n",
    "\n",
    "1. **Master** − It is the URL of the cluster it connects to.\n",
    "2. **appName** − Name of your job.\n",
    "3. **sparkHome** − Spark installation directory.\n",
    "4. **pyFiles** − The .zip or .py files to send to the cluster and add to the PYTHONPATH.\n",
    "5. **Environment** − Worker nodes environment variables.\n",
    "6. **batchSize** − The number of Python objects represented as a single Java object. Set 1 to disable batching, 0 to automatically choose the batch size based on object sizes, or -1 to **use an unlimited batch size.\n",
    "7. **Serializer** − RDD serializer.\n",
    "8. **Conf** − An object of L{SparkConf} to set all the Spark properties.\n",
    "9. **Gateway** − Use an existing gateway and JVM, otherwise initializing a new JVM.\n",
    "10. **JSC** − The JavaSparkContext instance.\n",
    "11. **profiler_cls** − A class of custom Profiler used to do profiling (the default is pyspark.profiler.BasicProfiler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a058676e-c66e-437f-95a2-02abafef3542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/30 09:19:02 WARN Utils: Your hostname, amandeep resolves to a loopback address: 127.0.1.1; using 192.168.195.129 instead (on interface ens33)\n",
      "22/06/30 09:19:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/30 09:19:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.195.129:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>session 01</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fca3003bdc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc = SparkSession.builder.appName('session 01').getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c133f5-09d8-434e-8309-0879086263a3",
   "metadata": {},
   "source": [
    "## 4. What is RDD in PySpark?\n",
    "\n",
    "Apache Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs:\n",
    "\n",
    "1. Parallelizing an existing collection in the driver program.\n",
    "2. Referencing a dataaset in an external storage system.\n",
    "\n",
    "Parallelized collections are created by calling `SparkContext`s `parallelize` method on an existing iterable or collection in the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e39e998-3722-473b-853d-1ebb02e74d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "shopping_list = sc.sparkContext.parallelize([\n",
    "    'bread', 'eggs', 'mangoes'\n",
    "])\n",
    "print(type(shopping_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f5320-bc19-4ea9-9927-531408957be3",
   "metadata": {},
   "source": [
    "The elements ofthe collection are copied to form a distributed dataset that can be operated on in parallel. One important parameter for parallel collections is the number of partitions to the cut the datasets into. Spark will run one task for each partition of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc931387-d6e8-40e5-839c-59660b5a658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "shopping_list = sc.sparkContext.parallelize([\n",
    "    'bread', 'eggs', 'mangoes', 'bananas', 'chicken', 'ketchup'\n",
    "], 3) # The RDD with the name shopping_list has 3 partitions.\n",
    "print(shopping_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6605c3-158c-47a2-8219-8a5f899d1a6e",
   "metadata": {},
   "source": [
    "## 4.2 How to read a `.txt` file as a PySpark RDD?\n",
    "\n",
    "The `textFile` method, just like the `parallelize` method takes an optional second argument for controlling the number of partitions of the file. Spark creates one partition for each block of file (Each block is 128 MB by default in HDFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dceb9474-a061-4986-a8c0-91e28cd51bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "text_rdd = sc.sparkContext.textFile('./data/rdd_text.txt')\n",
    "print(type(text_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1546633e-79b8-42a5-9479-aee05998c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/rdd_text.txt MapPartitionsRDD[127] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mankey\n",
      "Arbok\n",
      "Weedle\n",
      "Metapod\n",
      "Gloom\n",
      "Geodude\n",
      "Pikachu\n",
      "Evee\n",
      "Snorlax\n"
     ]
    }
   ],
   "source": [
    "text_rdd = sc.sparkContext.textFile('./data/rdd_text.txt', 3)\n",
    "print(text_rdd)\n",
    "text_rdd.foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc768e48-7b61-40de-bf82-b8e31ed9657b",
   "metadata": {},
   "source": [
    "Spark also lets you read a directory containing multiple small text file as RDDs. This can be done with the help of the `wholeTextFiles` method, which will return each of the files as (filename, file-content) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e70ac559-4d3f-48c0-aa15-abc2fe363cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('file:/home/amandeep/Projects/Learning-PySpark/data/text_files_folder/metal_bands.txt', 'Pantera\\nMetallica\\nSlayer\\nBlack Sabbath\\nDef Leppard\\nExodus\\nVenom\\nIron Maiden\\n')\n",
      "('file:/home/amandeep/Projects/Learning-PySpark/data/text_files_folder/matrix_characters.txt', 'Neo\\nMorphious\\nTrinity\\nAgent Smith\\n')\n",
      "('file:/home/amandeep/Projects/Learning-PySpark/data/text_files_folder/pokemon.txt', 'Mankey\\nArbok\\nWeedle\\nMetapod\\nGloom\\nGeodude\\nPikachu\\nEvee\\nSnorlax\\n')\n"
     ]
    }
   ],
   "source": [
    "rdd_files = sc.sparkContext.wholeTextFiles('./data/text_files_folder')\n",
    "print(type(rdd_files))\n",
    "rdd_files.foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c19c606-4019-45ec-808c-05683388e77e",
   "metadata": {},
   "source": [
    "## 5. What is a PySpark DataFrame?\n",
    "PySpark DataFrames are lazily evaluated and are implemented on top of RDDs. When Apache Spark transforms data, it does not immediately compute the transform but plans how to perform the computation later. \n",
    "\n",
    "A PySpark DataFrame can be created by passing:\n",
    "\n",
    "1. list of lists\n",
    "2. tuple/list of tuples/lists\n",
    "3. pandas DataFrame\n",
    "4. RDDs\n",
    "5. pyspark.sql.Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "260d3f21-d34a-420f-b724-b44411605e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|character|         description|\n",
      "+---------+--------------------+\n",
      "|      Neo|     The chosen one.|\n",
      "| Morpheus|The bald guy with...|\n",
      "|  Trinity|The badass woman ...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = sc.createDataFrame([\n",
    "    Row(character='Neo', description='The chosen one.'),\n",
    "    Row(character='Morpheus', description='The bald guy with cool glasses.'),\n",
    "    Row(character='Trinity', description='The badass woman character.')\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3736bad7-37e0-4f1b-a4ec-b7499c260fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Character|         Description|\n",
      "+---------+--------------------+\n",
      "|      Neo|     The chosen one.|\n",
      "| Morpheus|The bald guy with...|\n",
      "|  Trinity|The badass woman ...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sc.createDataFrame([\n",
    "    ('Neo', 'The chosen one.'),\n",
    "    ('Morpheus', 'The bald guy with cool glasses.'),\n",
    "    ('Trinity', 'The badass woman character.')\n",
    "], schema='Character String, Description String')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69771704-89be-4e85-a35d-b3989db35d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Character|         Description|\n",
      "+---------+--------------------+\n",
      "|      Neo|     The chosen one.|\n",
      "| Morpheus|The bald guy with...|\n",
      "|  Trinity|The badass woman ...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Character': ['Neo', 'Morpheus', 'Trinity'],\n",
    "    'Description': ['The chosen one.', 'The bald guy with cool glasses.', 'The badass woman character.']\n",
    "})\n",
    "df = sc.createDataFrame(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e74964db-beef-4af5-afef-8ca14f523821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Character|         Description|\n",
      "+---------+--------------------+\n",
      "|      Neo|     The chosen one.|\n",
      "| Morpheus|The bald guy with...|\n",
      "|  Trinity|The badass woman ...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.sparkContext.parallelize([\n",
    "    ('Neo', 'The chosen one.'),\n",
    "    ('Morpheus', 'The bald guy with cool glasses.'),\n",
    "    ('Trinity', 'The badass woman character.')\n",
    "])\n",
    "df = sc.createDataFrame(rdd, schema='Character String, Description String')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2ad40-b8bc-4858-bfe3-9e1d2b56be00",
   "metadata": {},
   "source": [
    "### 5.1 How to check the schema of a PySpark DataFrame? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc578707-0444-492d-beb7-0215c268dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Character: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea974927-8bd4-4eb1-8cb9-668a139abec3",
   "metadata": {},
   "source": [
    "### 5.2 How to read a `.csv` file as a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a9bb21f-d040-4692-843b-ae6f188793f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n",
      "| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n",
      "| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n",
      "| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n",
      "| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n",
      "| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n",
      "| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n",
      "| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n",
      "| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n",
      "| 20|          5.1|         3.8|          1.5|         0.3|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = sc.read.csv('./data/iris.csv', inferSchema=True, header=True)\n",
    "iris.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c09699-eb6f-43a4-95f3-d62dacf3fc45",
   "metadata": {},
   "source": [
    "### 5.3 How to check the shape of PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d64959b-31f7-4bca-8406-5c68a0170d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 6\n"
     ]
    }
   ],
   "source": [
    "print(iris.count(), len(iris.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21245e-3e26-4ee9-a306-1f64fd53e198",
   "metadata": {},
   "source": [
    "### 5.4 How to see the first 5 rows of a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b6a52fe-3864-4691-bda5-1cd034741618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=1, SepalLengthCm=5.1, SepalWidthCm=3.5, PetalLengthCm=1.4, PetalWidthCm=0.2, Species='Iris-setosa'),\n",
       " Row(Id=2, SepalLengthCm=4.9, SepalWidthCm=3.0, PetalLengthCm=1.4, PetalWidthCm=0.2, Species='Iris-setosa'),\n",
       " Row(Id=3, SepalLengthCm=4.7, SepalWidthCm=3.2, PetalLengthCm=1.3, PetalWidthCm=0.2, Species='Iris-setosa'),\n",
       " Row(Id=4, SepalLengthCm=4.6, SepalWidthCm=3.1, PetalLengthCm=1.5, PetalWidthCm=0.2, Species='Iris-setosa'),\n",
       " Row(Id=5, SepalLengthCm=5.0, SepalWidthCm=3.6, PetalLengthCm=1.4, PetalWidthCm=0.2, Species='Iris-setosa')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d921b74-876f-49a2-9d48-1f3742d4d207",
   "metadata": {},
   "source": [
    "### 5.5 How to see the last 5 rows of a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3534efd-0b43-4ce3-9d9f-89f905d80ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=146, SepalLengthCm=6.7, SepalWidthCm=3.0, PetalLengthCm=5.2, PetalWidthCm=2.3, Species='Iris-virginica'),\n",
       " Row(Id=147, SepalLengthCm=6.3, SepalWidthCm=2.5, PetalLengthCm=5.0, PetalWidthCm=1.9, Species='Iris-virginica'),\n",
       " Row(Id=148, SepalLengthCm=6.5, SepalWidthCm=3.0, PetalLengthCm=5.2, PetalWidthCm=2.0, Species='Iris-virginica'),\n",
       " Row(Id=149, SepalLengthCm=6.2, SepalWidthCm=3.4, PetalLengthCm=5.4, PetalWidthCm=2.3, Species='Iris-virginica'),\n",
       " Row(Id=150, SepalLengthCm=5.9, SepalWidthCm=3.0, PetalLengthCm=5.1, PetalWidthCm=1.8, Species='Iris-virginica')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16feaa5-0891-41ea-9e6d-a83ddaef8d8f",
   "metadata": {},
   "source": [
    "### 5.6 How to subset columns from a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13663f64-8413-4840-929c-c120470a93c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|SepalLengthCm|    Species|\n",
      "+-------------+-----------+\n",
      "|          5.1|Iris-setosa|\n",
      "|          4.9|Iris-setosa|\n",
      "|          4.7|Iris-setosa|\n",
      "+-------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(['SepalLengthCm', 'Species']).show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79a65a-5f46-4353-a37d-c5cc393a7194",
   "metadata": {},
   "source": [
    "### 5.7 How to subset rows from a PySpark DataFrame on the basis of a condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cacb8509-7d7f-45cc-a437-f97ab34e52f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in the PySpark DataFrame before filtering rows: 150\n",
      "Rows in the PySpark DataFrame after filtering rows: 50\n"
     ]
    }
   ],
   "source": [
    "print('Rows in the PySpark DataFrame before filtering rows:', iris.count())\n",
    "filtered_df = iris.filter(iris['Species']=='Iris-virginica')\n",
    "print('Rows in the PySpark DataFrame after filtering rows:', filtered_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5618b3c5-b6dc-4584-8a6c-541db811d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+--------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|       Species|\n",
      "+---+-------------+------------+-------------+------------+--------------+\n",
      "|101|          6.3|         3.3|          6.0|         2.5|Iris-virginica|\n",
      "|102|          5.8|         2.7|          5.1|         1.9|Iris-virginica|\n",
      "|103|          7.1|         3.0|          5.9|         2.1|Iris-virginica|\n",
      "|104|          6.3|         2.9|          5.6|         1.8|Iris-virginica|\n",
      "|105|          6.5|         3.0|          5.8|         2.2|Iris-virginica|\n",
      "|106|          7.6|         3.0|          6.6|         2.1|Iris-virginica|\n",
      "|108|          7.3|         2.9|          6.3|         1.8|Iris-virginica|\n",
      "|109|          6.7|         2.5|          5.8|         1.8|Iris-virginica|\n",
      "|110|          7.2|         3.6|          6.1|         2.5|Iris-virginica|\n",
      "|111|          6.5|         3.2|          5.1|         2.0|Iris-virginica|\n",
      "|112|          6.4|         2.7|          5.3|         1.9|Iris-virginica|\n",
      "|113|          6.8|         3.0|          5.5|         2.1|Iris-virginica|\n",
      "|115|          5.8|         2.8|          5.1|         2.4|Iris-virginica|\n",
      "|116|          6.4|         3.2|          5.3|         2.3|Iris-virginica|\n",
      "|117|          6.5|         3.0|          5.5|         1.8|Iris-virginica|\n",
      "|118|          7.7|         3.8|          6.7|         2.2|Iris-virginica|\n",
      "|119|          7.7|         2.6|          6.9|         2.3|Iris-virginica|\n",
      "|121|          6.9|         3.2|          5.7|         2.3|Iris-virginica|\n",
      "|123|          7.7|         2.8|          6.7|         2.0|Iris-virginica|\n",
      "|125|          6.7|         3.3|          5.7|         2.1|Iris-virginica|\n",
      "+---+-------------+------------+-------------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.filter((iris['Species']=='Iris-virginica') & (iris['PetalLengthCm']>5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dd8ea52-4dfd-40fc-988b-368025d87316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+---------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|        Species|\n",
      "+---+-------------+------------+-------------+------------+---------------+\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|    Iris-setosa|\n",
      "| 42|          4.5|         2.3|          1.3|         0.3|    Iris-setosa|\n",
      "| 54|          5.5|         2.3|          4.0|         1.3|Iris-versicolor|\n",
      "| 55|          6.5|         2.8|          4.6|         1.5|Iris-versicolor|\n",
      "| 56|          5.7|         2.8|          4.5|         1.3|Iris-versicolor|\n",
      "| 58|          4.9|         2.4|          3.3|         1.0|Iris-versicolor|\n",
      "| 59|          6.6|         2.9|          4.6|         1.3|Iris-versicolor|\n",
      "| 60|          5.2|         2.7|          3.9|         1.4|Iris-versicolor|\n",
      "| 61|          5.0|         2.0|          3.5|         1.0|Iris-versicolor|\n",
      "| 63|          6.0|         2.2|          4.0|         1.0|Iris-versicolor|\n",
      "| 64|          6.1|         2.9|          4.7|         1.4|Iris-versicolor|\n",
      "| 65|          5.6|         2.9|          3.6|         1.3|Iris-versicolor|\n",
      "| 68|          5.8|         2.7|          4.1|         1.0|Iris-versicolor|\n",
      "| 69|          6.2|         2.2|          4.5|         1.5|Iris-versicolor|\n",
      "| 70|          5.6|         2.5|          3.9|         1.1|Iris-versicolor|\n",
      "| 72|          6.1|         2.8|          4.0|         1.3|Iris-versicolor|\n",
      "| 73|          6.3|         2.5|          4.9|         1.5|Iris-versicolor|\n",
      "| 74|          6.1|         2.8|          4.7|         1.2|Iris-versicolor|\n",
      "| 75|          6.4|         2.9|          4.3|         1.3|Iris-versicolor|\n",
      "| 77|          6.8|         2.8|          4.8|         1.4|Iris-versicolor|\n",
      "+---+-------------+------------+-------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.filter(((iris['Species']=='Iris-virginica') & (iris['PetalLengthCm']>5)) | (iris['SepalWidthCm']<3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ff56b-607a-4cd0-a80a-d106e5b15462",
   "metadata": {},
   "source": [
    "### 5.8 How to generate summary statistics from a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d966a95f-c70b-490a-9063-66592f85b36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|                Id|     SepalLengthCm|       SepalWidthCm|     PetalLengthCm|      PetalWidthCm|       Species|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|               150|               150|                150|               150|               150|           150|\n",
      "|   mean|              75.5| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|          null|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|          null|\n",
      "|    min|                 1|               4.3|                2.0|               1.0|               0.1|   Iris-setosa|\n",
      "|    max|               150|               7.9|                4.4|               6.9|               2.5|Iris-virginica|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f47cb1-0217-4b7b-a694-94493511166d",
   "metadata": {},
   "source": [
    "### 5.9 How to add a new column to a PySpark DataFrame?\n",
    "\n",
    "#### 5.9.1 How to add a new column to a PySpark DataFrame that contains a constant value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d1a5845-c9fb-4134-84b0-fc4c30a531fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|         Code Writer|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|Amandeep Singh Kh...|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|Amandeep Singh Kh...|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|Amandeep Singh Kh...|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "iris_updated = iris.withColumn('Code Writer', lit('Amandeep Singh Khanna'))\n",
    "iris_updated.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b391780e-7324-4e25-8355-094d86694e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+--------------------+----------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|         Code Writer|My Coding Skills|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+----------------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|Amandeep Singh Kh...|              10|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|Amandeep Singh Kh...|              10|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|Amandeep Singh Kh...|              10|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_updated = iris_updated.withColumn('My Coding Skills', lit(10))\n",
    "iris_updated.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a5f65-3d6f-4f4f-9166-1819db6482fb",
   "metadata": {},
   "source": [
    "#### 5.9.2 How to add a new column to a PySpark DataFrame that is on the basis of a condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de302ee3-7a2e-431b-95de-cf9bd99bbc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+--------------------+----------------+--------------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|         Code Writer|My Coding Skills|  Conditional Column|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+----------------+--------------------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|Amandeep Singh Kh...|              10|Below Average Sep...|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|Amandeep Singh Kh...|              10|Below Average Sep...|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|Amandeep Singh Kh...|              10|Below Average Sep...|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "iris_updated = iris_updated.withColumn('Conditional Column', when(col('SepalLengthCm')>5.83, 'Above Average Sepal Length').otherwise('Below Average Sepal Length'))\n",
    "iris_updated.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aab7b9a3-2872-430a-bd61-2d9f17273dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+---------------+--------------------+----------------+--------------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|        Species|         Code Writer|My Coding Skills|  Conditional Column|\n",
      "+---+-------------+------------+-------------+------------+---------------+--------------------+----------------+--------------------+\n",
      "| 51|          7.0|         3.2|          4.7|         1.4|Iris-versicolor|Amandeep Singh Kh...|              10|Above Average Sep...|\n",
      "| 52|          6.4|         3.2|          4.5|         1.5|Iris-versicolor|Amandeep Singh Kh...|              10|Above Average Sep...|\n",
      "| 53|          6.9|         3.1|          4.9|         1.5|Iris-versicolor|Amandeep Singh Kh...|              10|Above Average Sep...|\n",
      "+---+-------------+------------+-------------+------------+---------------+--------------------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_updated.filter(iris_updated['Conditional Column'] == 'Above Average Sepal Length').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043c8d2-daf3-449a-8438-ff873bceab28",
   "metadata": {},
   "source": [
    "### 5.10 How to perform groupby operations of a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f254a8d-1bb5-493b-a5dc-4d5b78033178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+------------------+------------------+------------------+------------------+\n",
      "|        Species|avg(Id)|avg(SepalLengthCm)| avg(SepalWidthCm)|avg(PetalLengthCm)| avg(PetalWidthCm)|\n",
      "+---------------+-------+------------------+------------------+------------------+------------------+\n",
      "| Iris-virginica|  125.5| 6.587999999999998|2.9739999999999998|             5.552|             2.026|\n",
      "|    Iris-setosa|   25.5| 5.005999999999999|3.4180000000000006|             1.464|0.2439999999999999|\n",
      "|Iris-versicolor|   75.5|             5.936|2.7700000000000005|              4.26|1.3259999999999998|\n",
      "+---------------+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.groupBy(['Species']).avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9918813b-0d14-4883-b68f-f7e51bd837a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|        Species|avg(SepalLengthCm)|\n",
      "+---------------+------------------+\n",
      "| Iris-virginica| 6.587999999999998|\n",
      "|    Iris-setosa| 5.005999999999999|\n",
      "|Iris-versicolor|             5.936|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(['Species', 'SepalLengthCm']).groupby(['Species']).avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c2512-33b7-41e8-b25c-f5777b436de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
